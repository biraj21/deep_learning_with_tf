{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80664264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c16896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveDense:\n",
    "    \"\"\"\n",
    "    A fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_neurons, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        # randomly initialize weights in range [0, 0.1)\n",
    "        self.W = tf.Variable(\n",
    "            tf.random.uniform(\n",
    "                shape=(input_size, num_neurons),\n",
    "                minval=0,\n",
    "                maxval=0.1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # initialize biases to 0\n",
    "        self.b = tf.Variable(\n",
    "            tf.zeros(shape=(num_neurons,))\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Transform the inputs of this layer with weighted sum and activation.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        \"\"\"\n",
    "        Get weights and biases of this layer.\n",
    "        \"\"\"\n",
    "\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e25b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    \"\"\"\n",
    "    A neural network with sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        A forward pass through the network.\n",
    "        \"\"\"\n",
    "\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compile(self, optimizer, loss):\n",
    "        \"\"\"\n",
    "        Configure the network before training.\n",
    "        \"\"\"\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, inputs, labels, epochs, batch_size=128):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"{epoch})\")\n",
    "\n",
    "            batch_generator = BatchGenerator(inputs, labels)\n",
    "            for i in range(batch_generator.num_batches):\n",
    "                inputs_batch, labels_batch = batch_generator.next()\n",
    "                loss = self.one_training_step(inputs_batch, labels_batch)\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"  loss at batch {i}: {loss:.2f}\")\n",
    "\n",
    "    def one_training_step(self, inputs_batch, labels_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # record the operations performed on tensors within this scope\n",
    "\n",
    "            # make a forwords pass\n",
    "            predictions = self(inputs_batch)\n",
    "\n",
    "            # compute average loss of this batch\n",
    "            per_sample_losses = self.loss(labels_batch, predictions)\n",
    "            average_loss = tf.reduce_mean(per_sample_losses)\n",
    "\n",
    "        # compute gradients of loss wrt. network parameters with automatic differentiation\n",
    "        gradients = tape.gradient(average_loss, self.weights)\n",
    "\n",
    "        # update the parameters using gradients\n",
    "        self.update_weights(gradients, self.weights)\n",
    "\n",
    "        return average_loss\n",
    "\n",
    "    def update_weights(self, gradients, weights):\n",
    "        self.optimizer.apply_gradients(zip(gradients, weights))\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        \"\"\"\n",
    "        Get weights and biases of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2db2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    \"\"\"\n",
    "    For mini-batches of samples/inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, labels, batch_size=128):\n",
    "        assert len(inputs) == len(labels)\n",
    "\n",
    "        self.index = 0\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(inputs) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Load the next batch.\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = self.inputs[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size] \n",
    "        self.index += self.batch_size\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebff4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(-1, 28 * 28).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape(-1, 28 * 28).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ceca19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(28 * 28, 512, tf.nn.relu),\n",
    "    NaiveDense(512, 10, tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "#     optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80213942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0)\n",
      "  loss at batch 0: 6.48\n",
      "  loss at batch 100: 0.74\n",
      "  loss at batch 200: 0.39\n",
      "  loss at batch 300: 0.33\n",
      "  loss at batch 400: 0.54\n",
      "1)\n",
      "  loss at batch 0: 0.25\n",
      "  loss at batch 100: 0.40\n",
      "  loss at batch 200: 0.22\n",
      "  loss at batch 300: 0.31\n",
      "  loss at batch 400: 0.42\n",
      "2)\n",
      "  loss at batch 0: 0.18\n",
      "  loss at batch 100: 0.19\n",
      "  loss at batch 200: 0.21\n",
      "  loss at batch 300: 0.23\n",
      "  loss at batch 400: 0.29\n",
      "3)\n",
      "  loss at batch 0: 0.13\n",
      "  loss at batch 100: 0.11\n",
      "  loss at batch 200: 0.14\n",
      "  loss at batch 300: 0.13\n",
      "  loss at batch 400: 0.19\n",
      "4)\n",
      "  loss at batch 0: 0.08\n",
      "  loss at batch 100: 0.08\n",
      "  loss at batch 200: 0.09\n",
      "  loss at batch 300: 0.10\n",
      "  loss at batch 400: 0.14\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9691e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "126b1e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "matches = predictions == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa62553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3f31c08bb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(test_images[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df7d1807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a7714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
