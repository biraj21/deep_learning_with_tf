{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdbipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "metadata": {
        "id": "nGCKJ1C2RxcQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10_000)\n",
        "word_index = imdb.get_word_index()\n",
        "index_to_word = dict(\n",
        "    [(value, key) for key, value in word_index.items()]\n",
        ")"
      ],
      "metadata": {
        "id": "V7QpYeqgeGfy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_reviews(reviews, dims=10_000):\n",
        "    results = np.zeros((len(reviews), dims))\n",
        "    for i, review in enumerate(reviews):\n",
        "        for word_index in review:\n",
        "            results[i, word_index] = 1\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "gtEcL_EdoF6j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorize_reviews(train_data)\n",
        "x_test = vectorize_reviews(test_data)\n",
        "\n",
        "y_train = train_labels.astype(\"float32\")\n",
        "y_test = test_labels.astype(\"float32\")\n",
        "\n",
        "x_val = x_train[:10_000]\n",
        "y_val = y_train[:10_000]\n",
        "\n",
        "partial_x_train = x_train[10_000:]\n",
        "partial_y_train = y_train[10_000:]"
      ],
      "metadata": {
        "id": "A_Lp_Y6drXCR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    partial_x_train,\n",
        "    partial_y_train,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_val, y_val),\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "YOhEYYDzhjQy",
        "outputId": "7df80f71-bc6a-44e5-8b6b-333edde7076f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "30/30 - 2s - loss: 0.5296 - accuracy: 0.7821 - val_loss: 0.4115 - val_accuracy: 0.8600 - 2s/epoch - 82ms/step\n",
            "Epoch 2/10\n",
            "30/30 - 1s - loss: 0.3250 - accuracy: 0.9009 - val_loss: 0.3217 - val_accuracy: 0.8803 - 1s/epoch - 39ms/step\n",
            "Epoch 3/10\n",
            "30/30 - 1s - loss: 0.2358 - accuracy: 0.9262 - val_loss: 0.2837 - val_accuracy: 0.8916 - 1s/epoch - 43ms/step\n",
            "Epoch 4/10\n",
            "30/30 - 1s - loss: 0.1849 - accuracy: 0.9411 - val_loss: 0.2776 - val_accuracy: 0.8898 - 1s/epoch - 42ms/step\n",
            "Epoch 5/10\n",
            "30/30 - 1s - loss: 0.1519 - accuracy: 0.9507 - val_loss: 0.2828 - val_accuracy: 0.8872 - 1s/epoch - 41ms/step\n",
            "Epoch 6/10\n",
            "30/30 - 1s - loss: 0.1228 - accuracy: 0.9642 - val_loss: 0.3267 - val_accuracy: 0.8760 - 1s/epoch - 37ms/step\n",
            "Epoch 7/10\n",
            "30/30 - 1s - loss: 0.1052 - accuracy: 0.9679 - val_loss: 0.3016 - val_accuracy: 0.8846 - 978ms/epoch - 33ms/step\n",
            "Epoch 8/10\n",
            "30/30 - 1s - loss: 0.0846 - accuracy: 0.9764 - val_loss: 0.3208 - val_accuracy: 0.8826 - 1s/epoch - 37ms/step\n",
            "Epoch 9/10\n",
            "30/30 - 1s - loss: 0.0694 - accuracy: 0.9822 - val_loss: 0.3429 - val_accuracy: 0.8812 - 1s/epoch - 40ms/step\n",
            "Epoch 10/10\n",
            "30/30 - 1s - loss: 0.0580 - accuracy: 0.9849 - val_loss: 0.3609 - val_accuracy: 0.8793 - 1s/epoch - 40ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "LQm5LjmHsisQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=4,\n",
        "    batch_size=512,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSjMVokkx_jR",
        "outputId": "3f892ecc-7e6f-4ada-f802-1929c784a50e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "49/49 - 2s - loss: 0.4885 - accuracy: 0.8046 - 2s/epoch - 47ms/step\n",
            "Epoch 2/4\n",
            "49/49 - 2s - loss: 0.2455 - accuracy: 0.9108 - 2s/epoch - 34ms/step\n",
            "Epoch 3/4\n",
            "49/49 - 1s - loss: 0.1801 - accuracy: 0.9366 - 1s/epoch - 27ms/step\n",
            "Epoch 4/4\n",
            "49/49 - 1s - loss: 0.1442 - accuracy: 0.9518 - 1s/epoch - 26ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "9Pe0qmRby569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d766bf5-a58d-4d6f-a9a5-f415403f03a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.3107 - accuracy: 0.8796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_review(encoded):\n",
        "    return \" \".join(index_to_word.get(i - 3, \"?\") for i in encoded)\n",
        "\n",
        "def encode_review(review):\n",
        "    return np.array([1] + [word_index.get(word, -1) + 3 for word in review.split()])"
      ],
      "metadata": {
        "id": "LM4lB5Hzs-QU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_reviews = vectorize_reviews([\n",
        "    encode_review(\"this movie sucks\"),\n",
        "    encode_review(\"movie sucks this\"),\n",
        "    encode_review(\"movie sucks this movie sucks\"),\n",
        "    encode_review(\"i have never seen such a crap\"),\n",
        "    encode_review(\"i have never seen such a crap in my entire life\"),\n",
        "    encode_review(\"i have never watched such a masterpiece\"),\n",
        "    encode_review(\"i hate this movie\"),\n",
        "    encode_review(\"it's terrible\"),\n",
        "    encode_review(\"it's a really great movie\"),\n",
        "    encode_review(\"it was very boring movie totally not worth of time\"),\n",
        "    encode_review(\"very slow pace movie but everyone should watch it once\"),\n",
        "    encode_review(\"noice\"),\n",
        "])\n",
        "\n",
        "predictions = model.predict(my_reviews).reshape((-1,))\n",
        "for i, pred in enumerate(predictions, start=1):\n",
        "    print(f\"{i:2d}) Score: {pred:.6f}, Conclusion: {'+ve' if pred >= 0.5 else '-ve'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJT5ty3modI7",
        "outputId": "ffa21606-c1c9-4a2e-99e0-2d66e0dcc058"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1) Score: 0.365016, Conclusion: -ve\n",
            " 2) Score: 0.365016, Conclusion: -ve\n",
            " 3) Score: 0.365016, Conclusion: -ve\n",
            " 4) Score: 0.477219, Conclusion: -ve\n",
            " 5) Score: 0.465910, Conclusion: -ve\n",
            " 6) Score: 0.653365, Conclusion: +ve\n",
            " 7) Score: 0.512592, Conclusion: +ve\n",
            " 8) Score: 0.384642, Conclusion: -ve\n",
            " 9) Score: 0.696161, Conclusion: +ve\n",
            "10) Score: 0.358113, Conclusion: -ve\n",
            "11) Score: 0.700438, Conclusion: +ve\n",
            "12) Score: 0.514511, Conclusion: +ve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the first 3 examples (in `my_reviews`) and their predictions. Their predictions are esactly the same because our simple fully-connected neural network doesn't take into consideration the order of words and repeated words. There is no notion of context here. This should be obvious as we are just giving the vectors with all zeros except ones at word indices (from the `word_index` dictionary) as input."
      ],
      "metadata": {
        "id": "K9bNPxL7jRSR"
      }
    }
  ]
}